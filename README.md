# ğŸ’ƒğŸ¼ NCSNìœ¼ë¡œ CelebA ë°ì´í„°ì…‹ í›ˆë ¨

ë³¸ ë¬¸ì„œëŠ” 2019ë…„ ì œì•ˆëœ **Noise Conditional Score Network(NCSN)** ë¥¼ PyTorch ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬í˜„í•˜ê³ , ê·¸ ì‹¤í—˜ì  ë™ì‘ì„ í‰ê°€í•œ ê²°ê³¼ë¥¼ ê¸°ìˆ í•œë‹¤. ë³¸ êµ¬í˜„ì€ score-based generative modelingì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¥¼ ì¶©ì‹¤íˆ ë”°ë¥´ë©°, í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” Annealed Denoising Score Matching(DSM)ì„, ìƒ˜í”Œë§ ë‹¨ê³„ì—ì„œëŠ” Annealed Langevin Dynamics(ALD)ë¥¼ ì‚¬ìš©í•œë‹¤.

#### NCSN ìƒ˜í”Œë§ ë¯¸ë¦¬ë³´ê¸°

| ì´ˆê¸° ë¶„í¬ | `seed=42` | `seed=10` |
|------|------|------|
| $\mathcal{U}(-1,1)\in\mathbb{R}^{N\times H\times W}$ | <p align="center"><img src="https://velog.velcdn.com/images/lumerico284/post/c6a43ef5-3986-4483-acf5-59e54c13fdd2/image.gif" width="70%"/></p> | <p align="center"><img src="https://velog.velcdn.com/images/lumerico284/post/e8b3a26b-a1c1-47d9-bc2f-17ccf88c20cb/image.gif" width="70%"/></p> |
| $\mathcal{N}(\mathbf{0},\mathbf{I})\in\mathbb{R}^{N\times H\times W}$ | <p align="center"><img src="https://velog.velcdn.com/images/lumerico284/post/0d8a3a83-474c-49f1-b06f-31397425ab49/image.gif" width="70%"/></p> | <p align="center"><img src="https://velog.velcdn.com/images/lumerico284/post/e6c4c7ef-9545-4802-98cf-fbd955bb52b8/image.gif" width="70%"/></p> |

ë³¸ í”„ë¡œì íŠ¸ì˜ ëª©ì ì€ ì› ë…¼ë¬¸ì˜ ë°©ë²•ë¡ ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì¬í˜„í•˜ê³ , ì´ë¡ â€“êµ¬í˜„ ê°„ ëŒ€ì‘ ê´€ê³„ë¥¼ ëª…í™•íˆ ë“œëŸ¬ë‚´ëŠ” ì½”ë“œë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ë° ìˆë‹¤.

---


## 1ï¸âƒ£ ì´ë¡ ì  ë°°ê²½

Score ê¸°ë°˜ ìƒì„± ëª¨ë¸(score-based generative models)ì€ ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ ì¡°ê±´ì—ì„œ ë°ì´í„°ì˜ ë¡œê·¸ ë°€ë„(log-density)ì˜ ê¸°ìš¸ê¸°(score)ë¥¼ ê·¼ì‚¬í•¨ìœ¼ë¡œì¨ ë³µì¡í•œ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê°•ë ¥í•œ ì ‘ê·¼ë²•ì´ë‹¤. NCSNì€ ë…¸ì´ì¦ˆ ì¡°ê±´í™”ëœ score ë„¤íŠ¸ì›Œí¬ë¥¼ ì´ìš©í•´ ê° ë…¸ì´ì¦ˆ ë ˆë²¨ì—ì„œì˜ scoreë¥¼ ì¶”ì •í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Langevin dynamicsë¥¼ í†µí•´ ìƒˆë¡œìš´ ìƒ˜í”Œì„ ìƒì„±í•œë‹¤.

### í‘œê¸°(Notation)

ë³¸ ë¬¸ì„œì—ì„œëŠ” **ë²¡í„°/í…ì„œ**ë¥¼ **êµµì€ ê¸€ì”¨**ë¡œ, **ìŠ¤ì¹¼ë¼**ëŠ” ì¼ë°˜ ê¸€ì”¨ë¡œ í‘œê¸°í•œë‹¤.

- ë°ì´í„° ìƒ˜í”Œ(ì´ë¯¸ì§€): $\mathbf{x}\in \mathbb{R}^{C\times H\times W}$
- ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ: $\boldsymbol\epsilon\sim \mathcal{N}(\mathbf{0}, \mathbf{I})$
- ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¼(ìŠ¤ì¹¼ë¼): $\sigma > 0$
- ì´ì‚° ë…¸ì´ì¦ˆ ë ˆë²¨(ì •ìˆ˜): $y \in \{0,\ldots,K-1\}$

ì•„ë˜ì—ì„œ $\nabla_{\mathbf{x}}$ëŠ” $\mathbf{x}$ì— ëŒ€í•œ ê¸°ìš¸ê¸°(gradient)ë¥¼ ì˜ë¯¸í•œë‹¤.

### Score ì¶”ì • ë° ë…¸ì´ì¦ˆ ì¡°ê±´í™”

ë°ì´í„° ë¶„í¬ $p_{\text{data}}(\mathbf{x})$ì— ëŒ€í•´, NCSNì€ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ

$$
\mathbf{x}_\sigma = \mathbf{x} + \sigma \mathbf{z},\quad \mathbf{z}\sim \mathcal{N}(\mathbf{0},\mathbf{I})
$$

ë¥¼ ì‚¬ìš©í•œë‹¤. Score ë„¤íŠ¸ì›Œí¬ $s_\theta(\mathbf{x}_\sigma, \sigma)$ëŠ”

$$
\nabla_{\mathbf{x}_\sigma}\log p_\sigma(\mathbf{x}_\sigma)
$$

ë¥¼ ê·¼ì‚¬í•˜ë„ë¡ í•™ìŠµëœë‹¤. ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì—°ì†ì ì¸ $\sigma$ ëŒ€ì‹  ì´ì‚°ì  ë…¸ì´ì¦ˆ ë ˆë²¨ $y \in \{0,\ldots,K-1\}$ì„ ì‚¬ìš©í•œë‹¤.

ì—¬ê¸°ì„œ $p_\sigma(\mathbf{x}_\sigma)$ëŠ” ë°ì´í„°ì— ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ(ë¶„ì‚° $\sigma^2$)ë¥¼ ì„ì–´ ë§Œë“  ì£¼ë³€(marginal) ë¶„í¬ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ì»¤ë„(perturbation kernel)ë¡œ í‘œí˜„ëœë‹¤:

$$
q_\sigma(\mathbf{x}_\sigma\mid \mathbf{x})=
\mathcal{N}(\mathbf{x}_\sigma;\mathbf{x},\sigma^2\mathbf{I}),
\quad
p_\sigma(\mathbf{x}_\sigma)=
\int p_{\text{data}}(\mathbf{x})\,q_\sigma(\mathbf{x}_\sigma\mid \mathbf{x})\,d\mathbf{x}.
$$

NCSNì˜ ëª©í‘œëŠ” ë‹¤ì–‘í•œ $\sigma$ì—ì„œì˜ score $\nabla_{\mathbf{x}_\sigma}\log p_\sigma(\mathbf{x}_\sigma)$ë¥¼ ë™ì‹œì— ì¶”ì •í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•´ ì…ë ¥ì„ (ë…¸ì´ì¦ˆê°€ ì„ì¸) $\mathbf{x}_\sigma$ì™€ ë…¸ì´ì¦ˆ ë ˆë²¨ $y$ë¡œ ë‘ê³ , $y$ê°€ ê°€ë¦¬í‚¤ëŠ” $\sigma_y$ì—ì„œì˜ scoreë¥¼ ì¶œë ¥í•˜ë„ë¡ ë„¤íŠ¸ì›Œí¬ë¥¼ ì¡°ê±´í™”í•œë‹¤.

ë˜í•œ ì£¼ë³€ ë¶„í¬ì˜ scoreëŠ” **ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’**ìœ¼ë¡œë„ í‘œí˜„ëœë‹¤:

$$
\nabla_{\mathbf{x}_\sigma}\log p_\sigma(\mathbf{x}_\sigma)=\mathbb{E}\left[\nabla_{\mathbf{x}_\sigma}\log q_\sigma(\mathbf{x}_\sigma\mid \mathbf{x})\mid \mathbf{x}_\sigma\right]=
\mathbb{E}\left[\frac{\mathbf{x}-\mathbf{x}_\sigma}{\sigma^2}\bigg| \mathbf{x}_\sigma\right].
$$

ì¦‰, ë…¸ì´ì¦ˆê°€ ì„ì¸ ê´€ì¸¡ì¹˜ $\mathbf{x}_\sigma$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì›ë³¸ $\mathbf{x}$ë¥¼ í‰ê· ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ë˜ëŒë ¤ì•¼ í•˜ëŠ”ê°€ê°€ ê³§ scoreë¥¼ í•™ìŠµí•˜ëŠ” ë¬¸ì œë¡œ ì—°ê²°ëœë‹¤.

#### ë…¸ì´ì¦ˆ ì¡°ê±´í™”ê°€ í•„ìš”í•œ ì´ìœ 

í° $\sigma$ì—ì„œëŠ” ë¶„í¬ê°€ ë¶€ë“œëŸ¬ì›Œì ¸ scoreê°€ ë¹„êµì  ë‹¨ìˆœí•´ì§€ê³ , ì‘ì€ $\sigma$ì—ì„œëŠ” ë°ì´í„° ë§¤ë‹ˆí´ë“œ ê·¼ì²˜ì˜ ë³µì¡í•œ êµ¬ì¡°ë¥¼ ë” ì •êµí•˜ê²Œ ë³µì›í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ í•˜ë‚˜ì˜ ë„¤íŠ¸ì›Œí¬ê°€ ì—¬ëŸ¬ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¼ì„ ì²˜ë¦¬í•˜ë ¤ë©´ â€œí˜„ì¬ ë…¸ì´ì¦ˆ ë ˆë²¨ì´ ë¬´ì—‡ì¸ì§€â€ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì•Œë ¤ì£¼ëŠ” ì¡°ê±´($\sigma$ ë˜ëŠ” $y$)ì´ í•„ìš”í•˜ë‹¤.

### Annealed Denoising Score Matching(DSM)

DSM í•™ìŠµ ëª©ì  í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
\mathbb{E}_{\mathbf{x}\sim p_{\text{data}},\,\mathbf{z}\sim\mathcal{N}(\mathbf{0},\mathbf{I})}
\left[
\left\|
s_\theta(\mathbf{x} + \sigma \mathbf{z}, \sigma) + \frac{\mathbf{z}}{\sigma}
\right\|^2
\right].
$$

ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•´ ì´ë¥¼

$$
\left\|
\sigma\cdot s_\theta(\mathbf{x} + \sigma \mathbf{z}, \sigma) + \mathbf{z}
\right\|^2
$$

í˜•íƒœë¡œ ì¬ìŠ¤ì¼€ì¼ë§í•˜ì—¬ ì‚¬ìš©í•œë‹¤.

#### Score Matching ê´€ì 

ì´ìƒì ìœ¼ë¡œëŠ” ê° $\sigma$ì—ì„œì˜ true score $\nabla_{\mathbf{x}_\sigma}\log p_\sigma(\mathbf{x}_\sigma)$ë¥¼ ì§ì ‘ ë§ì¶”ëŠ” ë‹¤ìŒ ëª©ì ì„ ìƒê°í•  ìˆ˜ ìˆë‹¤:

$$
\mathbb{E}_{\mathbf{x}_\sigma\sim p_\sigma}
\left[
\left\| s_\theta(\mathbf{x}_\sigma,\sigma) - \nabla_{\mathbf{x}_\sigma}\log p_\sigma(\mathbf{x}_\sigma)\right\|^2
\right].
$$

ì´ëŠ” Fisher divergence(Score matching)ì™€ ì—°ê²°ë˜ì§€ë§Œ, $p_\sigma$ ìì²´ë¥¼ ëª¨ë¥´ê¸° ë•Œë¬¸ì— $\nabla\log p_\sigma$ë¥¼ ì§ì ‘ ê³„ì‚°í•  ìˆ˜ ì—†ë‹¤. DSMì€ $q_\sigma(\mathbf{x}_\sigma\mid\mathbf{x})$ì˜ ì„±ì§ˆ(ê°€ìš°ì‹œì•ˆ ì»¤ë„)ì„ ì´ìš©í•´ ì •ë‹µ íƒ€ê¹ƒì„ $-\mathbf{z}/\sigma$ í˜•íƒœë¡œ ë°”ê¿”ì„œ, ë™ì¼í•˜ê²Œ L2 íšŒê·€ ë¬¸ì œë¡œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë§Œë“ ë‹¤.

#### ìµœì í•´ê°€ $-\mathbf{z}/\sigma$ í˜•íƒœë¡œ ë‚˜íƒ€ë‚˜ëŠ” ì´ìœ 

ë…¸ì´ì¦ˆê°€ ì„ì¸ ê´€ì¸¡ì¹˜ $\mathbf{x}_\sigma$ë¥¼ ê³ ì •í–ˆì„ ë•Œ, ë‹¤ìŒì˜ conditional score identityê°€ ì„±ë¦½í•œë‹¤:

$$
\nabla_{\mathbf{x}_\sigma}\log q_\sigma(\mathbf{x}_\sigma\mid \mathbf{x})=
-\frac{\mathbf{x}_\sigma-\mathbf{x}}{\sigma^2}.
$$

ê·¸ëŸ°ë° $\mathbf{x}_\sigma=\mathbf{x}+\sigma\mathbf{z}$ì´ë¯€ë¡œ $\mathbf{x}_\sigma-\mathbf{x}=\sigma\mathbf{z}$, ë”°ë¼ì„œ

$$
\nabla_{\mathbf{x}_\sigma}\log q_\sigma(\mathbf{x}_\sigma\mid \mathbf{x})=
-\frac{\mathbf{z}}{\sigma}.
$$

DSMì€ ì£¼ë³€ ë¶„í¬ $p_\sigma(\mathbf{x}_\sigma)$ì˜ scoreë¥¼ ì§ì ‘ ê³„ì‚°í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ, ìœ„ì˜ ì¡°ê±´ë¶€ í•­ì„ ì´ìš©í•´ scoreë¥¼ í•™ìŠµí•œë‹¤. ì¦‰, $\mathbf{x}_\sigma$ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ë„¤íŠ¸ì›Œí¬ê°€ $-\mathbf{z}/\sigma$ì— ê°€ê¹Œìš´ ê°’ì„ ë‚´ë„ë¡ ìœ ë„í•˜ë©´, ê²°ê³¼ì ìœ¼ë¡œ $\nabla_{\mathbf{x}_\sigma}\log p_\sigma(\mathbf{x}_\sigma)$ë¥¼ ì˜ ê·¼ì‚¬í•˜ë„ë¡ í•™ìŠµëœë‹¤ëŠ” ê²ƒì´ í•µì‹¬ ì•„ì´ë””ì–´ì´ë‹¤.

#### Annealed(ë‹¤ì¤‘ ìŠ¤ì¼€ì¼) DSM

ì‹¤ì œë¡œëŠ” ë‹¨ì¼ $\sigma$ê°€ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ $\{\sigma_i\}_{i=1}^{K}$ë¥¼ ì‚¬ìš©í•œë‹¤. NCSNì—ì„œëŠ” $y$ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ $\sigma=\sigma_y$ë¥¼ ì„ íƒí•˜ê³ , ê·¸ ìŠ¤ì¼€ì¼ì—ì„œì˜ DSM ì†ì‹¤ì„ í‰ê· ë‚¸ë‹¤:

$$
\mathbb{E}_{y}\;
\mathbb{E}_{\mathbf{x},\mathbf{z}}
\left[
\left\|
\sigma_y\, s_\theta(\mathbf{x}+\sigma_y\mathbf{z}, y) + \mathbf{z}
\right\|^2
\right].
$$

ë˜í•œ êµ¬í˜„ ê´€ì ì—ì„œëŠ” **scoreì˜ í¬ê¸°(scale)**ê°€ $\sigma$ì— ë”°ë¼ í¬ê²Œ ë‹¬ë¼ì§€ëŠ” ê²½í–¥ì´ ìˆì–´(íŠ¹íˆ ì‘ì€ $\sigma$ì—ì„œ ë” í° ë³€í™”ê°€ í•„ìš”), ì†ì‹¤ì—ì„œ $\sigma$ë¡œ ì¬ìŠ¤ì¼€ì¼ë§(ìœ„ ì‹)í•˜ê±°ë‚˜ ëª¨ë¸ ì¶œë ¥ì— $\sigma^{-1}$ë¥¼ ë°˜ì˜í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì„ ì•ˆì •í™”í•œë‹¤.

### ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„(Noise Schedule)

ë…¸ì´ì¦ˆ ë ˆë²¨ì€ ë³´í†µ log-spaceì—ì„œ ë“±ê°„ê²©ìœ¼ë¡œ ë°°ì¹˜í•œ ë’¤ ì§€ìˆ˜ë¡œ ë˜ëŒë¦¬ëŠ” ë°©ì‹(geometric progression)ì„ ì‚¬ìš©í•œë‹¤. ì¦‰ $\sigma_{\max}=\sigma_{\text{begin}}$, $\sigma_{\min}=\sigma_{\text{end}}$ì— ëŒ€í•´

$$
\sigma_i=
\exp\Big(\log\sigma_{\max} + \frac{i}{K-1}\big(\log\sigma_{\min}-\log\sigma_{\max}\big)\Big),
\quad i=0,\ldots,K-1.
$$

ì´ë ‡ê²Œ í•˜ë©´ í° ë…¸ì´ì¦ˆ êµ¬ê°„ê³¼ ì‘ì€ ë…¸ì´ì¦ˆ êµ¬ê°„ì„ ëª¨ë‘ ì•ˆì •ì ìœ¼ë¡œ ì»¤ë²„í•  ìˆ˜ ìˆê³ , annealing ê³¼ì •ì—ì„œ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥¸ scoreë¥¼ ì ì§„ì ìœ¼ë¡œ í™œìš©í•˜ê¸°ê°€ ì‰¬ì›Œì§„ë‹¤. ë³¸ êµ¬í˜„ì—ì„œëŠ” ì´ì‚° ë ˆë²¨ $y=i$ì— ëŒ€í•´ $\sigma=\sigma_i$ë¥¼ ëŒ€ì‘ì‹œí‚¤ë©°, í•™ìŠµ ì‹œì—ëŠ” ë°°ì¹˜ë§ˆë‹¤ $y$ë¥¼ ê· ë“± ìƒ˜í”Œë§í•´ ë‹¤ì–‘í•œ ë…¸ì´ì¦ˆ ë ˆë²¨ì„ ê³ ë¥´ê²Œ í•™ìŠµí•œë‹¤.

### Annealed Langevin Dynamics(ALD) ìƒ˜í”Œë§

ê³ ë…¸ì´ì¦ˆ(í° $\sigma$)ì—ì„œ ì‹œì‘í•˜ì—¬ ì ì°¨ ë‚®ì€ $\sigma$ë¡œ ì´ë™í•˜ë©´ì„œ ë‹¤ìŒê³¼ ê°™ì€ Langevin ì—…ë°ì´íŠ¸ë¥¼ ë°˜ë³µ ìˆ˜í–‰í•œë‹¤:

$$
\mathbf{x} \leftarrow \mathbf{x} + \alpha\, s_\theta(\mathbf{x}, \sigma) + \sqrt{2\alpha}\,\boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon}\sim\mathcal{N}(\mathbf{0},\mathbf{I})
$$

ìŠ¤í… í¬ê¸°ëŠ”

$$
\alpha_i = \eta\left( \frac{\sigma_i}{\sigma_{\min}} \right)^2
$$

ë¡œ ì¡°ì •í•œë‹¤.

#### Scoreë¥¼ ë”°ë¼ í™•ë¥ ì´ ë†’ì€ ìª½ìœ¼ë¡œ ì´ë™

Score $\nabla_{\mathbf{x}}\log p(\mathbf{x})$ëŠ” í˜„ì¬ ìœ„ì¹˜ $\mathbf{x}$ì—ì„œ ë¡œê·¸ ë°€ë„ì˜ ì¦ê°€ ë°©í–¥ì„ ê°€ë¦¬í‚¨ë‹¤. ë”°ë¼ì„œ ì—…ë°ì´íŠ¸

$$
\mathbf{x}\leftarrow \mathbf{x} + \alpha\, s_\theta(\mathbf{x},\sigma)
$$

ëŠ” (ê·¼ì‚¬ëœ) score ë°©í–¥ìœ¼ë¡œ ì´ë™í•´ ë” ë†’ì€ í™•ë¥  ì§ˆëŸ‰ ì˜ì—­ìœ¼ë¡œ ìƒ˜í”Œì„ ë°€ì–´ë„£ê³ , $\sqrt{2\alpha}\boldsymbol{\epsilon}$ í•­ì€ íƒìƒ‰ì„ ìœ„í•œ ëœë¤ì„±ì„ ì œê³µí•œë‹¤. í° $\sigma$ì—ì„œëŠ” ë¶„í¬ê°€ ë§¤ë„ëŸ¬ì›Œ ì „ì—­ì ì¸ êµ¬ì¡°ë¥¼ ì¡ê¸° ì‰½ê³ , ì‘ì€ $\sigma$ë¡œ ê°ˆìˆ˜ë¡ ì„¸ë¶€ ì§ˆê°ì„ ì •êµí•˜ê²Œ ë³´ì •í•˜ê²Œ ëœë‹¤(annealing).

#### ì´ˆê¸°í™”ì™€ ê°’ í•œì •

ì‹¤ë¬´ì ìœ¼ë¡œëŠ” ì´ˆê¸° $\mathbf{x}$ë¥¼ **ê· ë“±ë¶„í¬** ë˜ëŠ” **ì •ê·œë¶„í¬**ì—ì„œ ìƒ˜í”Œë§í•œë‹¤. ë˜í•œ ì´ë¯¸ì§€ ì •ê·œí™” ë²”ìœ„ê°€ $[-1,1]$ì¸ ê²½ìš°, ì—…ë°ì´íŠ¸ ê³¼ì •ì—ì„œ ê°’ì´ í­ì£¼í•˜ì§€ ì•Šë„ë¡ ê° ìŠ¤í…ë§ˆë‹¤ $\mathbf{x}$ë¥¼ $[-1,1]$ë¡œ í´ë¨í”„í•˜ëŠ” ì„ íƒì„ ìì£¼ ì‚¬ìš©í•œë‹¤.

#### SDE ê´€ì 

ì´ìƒì ì¸ ê²½ìš°(ì •í™•í•œ scoreë¥¼ ì•ˆë‹¤ê³  ê°€ì •)ì—ëŠ” ë‹¤ìŒì˜ Langevin SDEê°€ ëª©í‘œ ë¶„í¬ë¥¼ stationary distributionìœ¼ë¡œ ê°–ëŠ”ë‹¤:

$$
d\mathbf{x}_t = \nabla_{\mathbf{x}}\log p(\mathbf{x}_t)\,dt + \sqrt{2}\,d\mathbf{w}_t,
$$

ì—¬ê¸°ì„œ $\mathbf{w}_t$ëŠ” ë¸Œë¼ìš´ ìš´ë™(Brownian motion)ì´ë‹¤. ì´ë¥¼ ì˜¤ì¼ëŸ¬-ë§ˆë£¨ì•¼ë§ˆ(Eulerâ€“Maruyama)ë¡œ ì´ì‚°í™”í•˜ë©´

$$
\mathbf{x}\leftarrow \mathbf{x} + \alpha\,\nabla_{\mathbf{x}}\log p(\mathbf{x}) + \sqrt{2\alpha}\,\boldsymbol{\epsilon}
$$

ì„ ì–»ê³ , ì‹¤ì œë¡œëŠ” $\nabla_{\mathbf{x}}\log p(\mathbf{x})$ ëŒ€ì‹  ê·¼ì‚¬ì¹˜ $s_\theta(\mathbf{x},\sigma)$ë¥¼ ì‚¬ìš©í•œë‹¤. NCSNì—ì„œëŠ” $p(\mathbf{x})$ ëŒ€ì‹  $p_\sigma(\mathbf{x})$ë¥¼ ë‹¤ë£¨ë©°, $\sigma$ë¥¼ í° ê°’ì—ì„œ ì‘ì€ ê°’ìœ¼ë¡œ ì ì°¨ ë‚®ì¶°ê°€ë©°(annealing) ìƒ˜í”Œì„ *ê±°ì¹œ êµ¬ì¡° $\to$ ì„¸ë¶€ êµ¬ì¡°* ìˆœì„œë¡œ ì •ì œí•œë‹¤.

---

## 2ï¸âƒ£ êµ¬í˜„ ì„¸ë¶€ ë‚´ìš©

### Conditional Instance Normalization

ë…¸ì´ì¦ˆ ì¡°ê±´í™”ë¥¼ ìœ„í•´ ë‹¤ìŒ í˜•íƒœì˜ CINì„ ì‚¬ìš©í•œë‹¤:

$$
\text{CIN}(h, y) = \gamma_y \odot \text{IN}(h) + \beta_y,
$$

ì—¬ê¸°ì„œ $\gamma_y, \beta_y$ëŠ” ë…¸ì´ì¦ˆ ë ˆë²¨ $y$ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµë˜ëŠ” íŒŒë¼ë¯¸í„°ì´ë‹¤.

êµ¬í˜„ì—ì„œëŠ” `InstanceNorm2d(affine=False)`ë¡œ $\text{IN}(h)$ë¥¼ ê³„ì‚°í•˜ê³ , `Embedding(num_classes, 2*C)`ë¡œ ê° ë…¸ì´ì¦ˆ ë ˆë²¨ì˜ $(\gamma_y,\beta_y)$ë¥¼ ìƒì„±í•œë‹¤. ë˜í•œ ì´ˆê¸° ìƒíƒœì—ì„œ $\gamma_y=1,\ \beta_y=0$ì´ ë˜ë„ë¡ ì„ë² ë”© ê°€ì¤‘ì¹˜ë¥¼ ì„¤ì •í•´(ì •ê·œí™”ë§Œ ì ìš©ëœ ìƒíƒœë¡œ ì‹œì‘) í•™ìŠµì„ ì•ˆì •í™”í•œë‹¤.

```python
# src/model.py
class ConditionalInstanceNorm2d(nn.Module):
    def __init__(self, num_features: int, num_classes: int, eps: float = 1e-5) -> None:
        super().__init__()
        self.num_features = num_features
        self.norm = nn.InstanceNorm2d(num_features, affine=False, eps=eps)
        self.embed = nn.Embedding(num_classes, num_features * 2)

        with torch.no_grad():
            self.embed.weight[:, :num_features].fill_(1.0)   # gamma init
            self.embed.weight[:, num_features:].zero_()      # beta init

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        if y.dtype != torch.long:
            y = y.long()
        h = self.norm(x)
        gamma_beta = self.embed(y)
        gamma, beta = torch.chunk(gamma_beta, 2, dim=1)
        gamma = gamma.view(-1, self.num_features, 1, 1)
        beta = beta.view(-1, self.num_features, 1, 1)
        return h * gamma + beta
```

### Score Network êµ¬ì¡°

ë„¤íŠ¸ì›Œí¬ ë°±ë³¸ì€ RefineNet ìŠ¤íƒ€ì¼ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì¶œë ¥ì€ ì´ë¡ ì‹ê³¼ ì¼ì¹˜í•˜ë„ë¡ $\sigma^{-1}$ ë°°ìœ¨ ì¡°ì •ì´ ì´ë£¨ì–´ì§„ë‹¤. ì´ëŠ” ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¼ ê°„ ì¼ê´€ëœ í•™ìŠµì„ ë•ëŠ”ë‹¤.

ì•„ë˜ êµ¬í˜„ì—ì„œ `labels=y`ëŠ” ì´ë¡  íŒŒíŠ¸ì˜ ì´ì‚° ë…¸ì´ì¦ˆ ë ˆë²¨($y\in\{0,\dots,K-1\}$)ì— í•´ë‹¹í•˜ë©°, ë‚´ë¶€ì˜ ì—¬ëŸ¬ ë¸”ë¡(ì˜ˆ: `RCUBlock`, `RefineBlock`)ì€ CINì„ í†µí•´ ë™ì¼í•œ íŠ¹ì„± ë§µì„ ì„œë¡œ ë‹¤ë¥¸ ë…¸ì´ì¦ˆ ì¡°ê±´ìœ¼ë¡œ ë³€ì¡°í•œë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ `scale_by_sigma=True`ì¸ ê²½ìš° ëª¨ë¸ ì¶œë ¥ `out`ì„ `out / sigma_y`ë¡œ ë‚˜ëˆ ì„œ, ë„¤íŠ¸ì›Œí¬ê°€ $\sigma$ì— ë”°ë¼ ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥¸ scoreë¥¼ ì§ì ‘ í•™ìŠµí•˜ëŠ” ë¶€ë‹´ì„ ì¤„ì¸ë‹¤. ì´ ìŠ¤ì¼€ì¼ë§ì€ ì•„ë˜ DSM ì†ì‹¤ì—ì„œ ë‹¤ì‹œ $\sigma$ë¥¼ ê³±í•´($\sigma s_\theta(\cdot,\sigma)$) ì´ë¡ ì‹ê³¼ ì •í™•íˆ ëŒ€ì‘ëœë‹¤.

```python
# src/model.py
class NCSN(nn.Module):
    def __init__(
        self,
        in_channels: int = 3,
        nf: int = 128,
        num_classes: int = 10,
        dilations: Sequence[int] = (1, 2, 4, 8),
        scale_by_sigma: bool = True,
    ) -> None:
        super().__init__()
        if len(dilations) != 4:
            raise ValueError("Expected 4 dilation values (for 4 RefineNet stages).")

        self.in_channels = in_channels
        self.nf = nf
        self.num_classes = num_classes
        self.scale_by_sigma = bool(scale_by_sigma)

        self.register_buffer("sigmas", torch.empty(num_classes), persistent=False)

        self.begin_conv = nn.Conv2d(in_channels, nf, kernel_size=3, stride=1, padding=1)

        self.stage1 = RCUBlock(nf, num_classes, num_units=2, dilation=dilations[0])
        self.stage2 = RCUBlock(nf, num_classes, num_units=2, dilation=dilations[1])
        self.stage3 = RCUBlock(nf, num_classes, num_units=2, dilation=dilations[2])
        self.stage4 = RCUBlock(nf, num_classes, num_units=2, dilation=dilations[3])

        self.refine4 = RefineBlock([nf], nf, num_classes)
        self.refine3 = RefineBlock([nf, nf], nf, num_classes)
        self.refine2 = RefineBlock([nf, nf], nf, num_classes)
        self.refine1 = RefineBlock([nf, nf], nf, num_classes)

        self.end_norm = ConditionalInstanceNorm2d(nf, num_classes)
        self.end_act = nn.ELU()
        self.end_conv = nn.Conv2d(nf, in_channels, kernel_size=3, stride=1, padding=1)

        self.apply(self._init_weights)

    @staticmethod
    def _init_weights(m: nn.Module) -> None:
        if isinstance(m, nn.Conv2d):
            nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                nn.init.zeros_(m.bias)

    def forward(self, x: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
        if labels.dim() != 1:
            labels = labels.view(-1)

        h = self.begin_conv(x)
        h1 = self.stage1(h, labels)
        h2 = self.stage2(h1, labels)
        h3 = self.stage3(h2, labels)
        h4 = self.stage4(h3, labels)

        r4 = self.refine4([h4], labels)
        r3 = self.refine3([h3, r4], labels)
        r2 = self.refine2([h2, r3], labels)
        r1 = self.refine1([h1, r2], labels)

        out = self.end_conv(self.end_act(self.end_norm(r1, labels)))
        if self.scale_by_sigma:
            used_sigmas = self.sigmas[labels].view(-1, 1, 1, 1)
            out = out / used_sigmas
        return out

    @torch.no_grad()
    def set_sigmas(self, sigmas: torch.Tensor) -> None:
        if sigmas.dim() != 1:
            raise ValueError("sigmas must be 1-D.")
        if sigmas.numel() != self.num_classes:
            raise ValueError(
                f"sigmas length ({sigmas.numel()}) must match num_classes ({self.num_classes})."
            )
        self.sigmas.copy_(sigmas.to(self.sigmas.device, dtype=self.sigmas.dtype))
```

### DSM ì†ì‹¤ í•¨ìˆ˜

í•™ìŠµ ê³¼ì •ì—ì„œëŠ” ê° ë°°ì¹˜ë§ˆë‹¤ ë¬´ì‘ìœ„ ë…¸ì´ì¦ˆ ë ˆë²¨ê³¼ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆë¥¼ ìƒ˜í”Œë§í•˜ì—¬ DSM ì†ì‹¤ì„ ê³„ì‚°í•œë‹¤. êµ¬í˜„ì€ ì´ë¡ ì‹ê³¼ ì™„ì „íˆ ì¼ì¹˜í•œë‹¤.

ì´ë¡  íŒŒíŠ¸ì˜ ì¬ìŠ¤ì¼€ì¼ë§ëœ DSM ëª©ì í•¨ìˆ˜

$$
\left\| \sigma s_\theta(x + \sigma z, \sigma) + z \right\|^2
$$

ë¥¼ ê·¸ëŒ€ë¡œ ê³„ì‚°í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” ë°°ì¹˜ë§ˆë‹¤ `labels=y`ë¥¼ ìƒ˜í”Œë§í•´ $\sigma_y$ë¥¼ ì„ íƒí•˜ê³ , `perturbed = x + sigma_y * noise`ë¡œ $x+\sigma z$ë¥¼ ë§Œë“¤ë©°, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ score `score = s_\theta(perturbed, y)`ì— $\sigma_y$ë¥¼ ê³±í•œ ë’¤ ë…¸ì´ì¦ˆ $z$ì™€ì˜ ì œê³±ì˜¤ì°¨ë¥¼ ì·¨í•œë‹¤.

```python
# src/losses/dsm.py
def annealed_dsm_loss(model, x: torch.Tensor, sigmas: torch.Tensor):
    batch = x.shape[0]
    device = x.device
    labels = torch.randint(0, sigmas.shape[0], (batch,), device=device, dtype=torch.long)
    used_sigmas = sigmas[labels].view(batch, 1, 1, 1)

    noise = torch.randn_like(x)
    perturbed = x + used_sigmas * noise
    score = model(perturbed, labels)

    loss = torch.mean(torch.sum((score * used_sigmas + noise) ** 2, dim=(1, 2, 3)))
    return loss, labels
```

### Langevin ìƒ˜í”Œë§ ì ˆì°¨

ìƒ˜í”ŒëŸ¬ëŠ” ê° ë…¸ì´ì¦ˆ ë ˆë²¨ë³„ë¡œ ì—¬ëŸ¬ Langevin ìŠ¤í…ì„ ìˆ˜í–‰í•˜ë©°, ì§„í–‰ ìƒí™©ì€ tqdmìœ¼ë¡œ ì‹œê°í™”ëœë‹¤. ìƒ˜í”Œì€ ê³ ë…¸ì´ì¦ˆì—ì„œ ì‹œì‘í•´ ì ì°¨ ì €ë…¸ì´ì¦ˆ ë‹¨ê³„ë¡œ ì´ë™í•˜ë©´ì„œ ìƒì„±ëœë‹¤.

ALD ì—…ë°ì´íŠ¸ëŠ” ì½”ë“œì—ì„œ `x = x + step_size * grad + sqrt(2*step_size) * noise`ë¡œ êµ¬í˜„ë˜ë©°,

$$
x \leftarrow x + \alpha s_\theta(x, \sigma) + \sqrt{2\alpha}\varepsilon
$$

ìŠ¤í… í¬ê¸°ëŠ” `step_size = step_lr * (sigma / sigmas[-1]) ** 2`ë¡œ ëŒ€ì‘ëœë‹¤(ì—¬ê¸°ì„œ `sigmas[-1]`ê°€ $\sigma_{\min}$).

$$
\alpha_i=\eta\left(\frac{\sigma_i}{\sigma_{\min}}\right)^2
$$

```python
# src/runners/scorenet_runner.py
@torch.no_grad()
def annealed_langevin_dynamics(
    model,
    sigmas: torch.Tensor,
    n_samples: int,
    image_size: int,
    in_channels: int,
    n_steps_each: int,
    step_lr: float,
    device: torch.device,
    clamp: bool = True,
    denoise: bool = False,
    init: Optional[torch.Tensor] = None,
    init_distribution: str = "uniform",
) -> torch.Tensor:
    model.eval()
    sigmas = sigmas.to(device)

    if init is None:
        if init_distribution == "uniform":
            x = torch.empty(n_samples, in_channels, image_size, image_size, device=device).uniform_(-1.0, 1.0)
        else:
            x = torch.randn(n_samples, in_channels, image_size, image_size, device=device)
    else:
        x = init.to(device)

    for i, sigma in enumerate(sigmas):
        labels = torch.full((n_samples,), i, device=device, dtype=torch.long)
        step_size = step_lr * (sigma / sigmas[-1]) ** 2

        for _ in range(n_steps_each):
            grad = model(x, labels)
            noise = torch.randn_like(x)
            x = x + step_size * grad + torch.sqrt(2.0 * step_size) * noise
            if clamp:
                x = x.clamp(-1.0, 1.0)

    if denoise:
        last_label = torch.full((n_samples,), sigmas.shape[0] - 1, device=device, dtype=torch.long)
        x = x + (sigmas[-1] ** 2) * model(x, last_label)
        if clamp:
            x = x.clamp(-1.0, 1.0)

    return x
```

### í•™ìŠµ ë£¨í”„

í•™ìŠµ ë£¨í”„ëŠ” ë°°ì¹˜ ë‹¨ìœ„ ì§„í–‰ ìƒí™©(loss, running loss, epoch í‰ê· )ì„ ëª¨ë‘ ê¸°ë¡í•˜ë©°, ì²´í¬í¬ì¸íŠ¸ì—ëŠ” optimizer ìƒíƒœ ë° running lossê°€ í¬í•¨ëœë‹¤.

í•™ìŠµ ë£¨í”„ëŠ” ì„¤ì • íŒŒì¼ì—ì„œ $\{\sigma_i\}_{i=1}^K$ë¥¼ ë§Œë“¤ê³ , `annealed_dsm_loss`ë¡œ ì´ë¡ ì‹ì˜ DSM ì†ì‹¤ì„ ê³„ì‚°í•˜ë©°, ì£¼ê¸°ì ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸/ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•œë‹¤. ì¦‰, ì´ë¡ ì‹(DSM, ALD) $\to$ ì½”ë“œ í•¨ìˆ˜ í˜¸ì¶œì´ `train()`ì—ì„œ ê·¸ëŒ€ë¡œ ì§ê²°ëœë‹¤.

```python
# src/main.py (ë°œì·Œ)
sigmas = make_sigmas(
    sigma_begin=float(cfg["model"].get("sigma_begin", 1.0)),
    sigma_end=float(cfg["model"].get("sigma_end", 0.01)),
    num_scales=int(cfg["model"].get("num_scales", 10)),
).to(device)

model = NCSN(
    in_channels=int(cfg["model"].get("in_channels", 3)),
    nf=int(cfg["model"].get("nf", 128)),
    num_classes=int(cfg["model"].get("num_scales", 10)),
    dilations=tuple(cfg["model"].get("dilations", (1, 2, 4, 8))),
    scale_by_sigma=bool(cfg["model"].get("scale_by_sigma", True)),
).to(device)
model.set_sigmas(sigmas)

for x in loader:
    x = x.to(device)
    optimizer.zero_grad(set_to_none=True)
    loss, _ = annealed_dsm_loss(model, x, sigmas)
    loss.backward()
    grad_clip = cfg["training"].get("grad_clip", None)
    if grad_clip is not None:
        torch.nn.utils.clip_grad_norm_(model.parameters(), float(grad_clip))
    optimizer.step()

samples = annealed_langevin_dynamics(
    model=model,
    sigmas=sigmas,
    n_samples=int(cfg["sampling"].get("n_samples", 64)),
    image_size=int(cfg["data"].get("image_size", 32)),
    in_channels=int(cfg["model"].get("in_channels", 3)),
    n_steps_each=int(cfg["sampling"].get("n_steps_each", 100)),
    step_lr=float(cfg["sampling"].get("step_lr", 2e-5)),
    device=device,
)
```

---

## 4ï¸âƒ£ ì‚¬ìš© ë°©ë²•

#### í•™ìŠµ ì‹¤í–‰

```bash
python -u src/main.py --dataset celeba --mode train
python -u src/main.py --dataset mnist --mode train
```

#### ì²´í¬í¬ì¸íŠ¸ ì¬ê°œ

```bash
python -u src/main.py --dataset mnist --mode train --resume latest
```

#### ìƒ˜í”Œë§

```bash
python -u src/main.py --dataset mnist --mode sample --ckpt latest --out out/sample.png
```

#### GIF ìƒì„±

```bash
python -u src/make_sampling_gif.py --dataset mnist --frames 50 --seed 123
```

#### Loss ê³¡ì„  ì‹œê°í™”

```bash
python -u src/plot_running_losses.py --out_dir out --ckpt latest
```

---

## 5ï¸âƒ£ ì‹¤í—˜

### ëª¨ë¸ êµ¬ì¡°

- ì´ íŒŒë¼ë¯¸í„° ìˆ˜: $3,176,067$
- MACs: $3.1\times10^9$
- ìˆœì „íŒŒ/ì—­ì „íŒŒ í¬ê¸°: `43.63 MB`
- ì „ì²´ ëª¨ë¸ í¬ê¸°: `56.19 MB`

<p align="center">
  <img src="https://velog.velcdn.com/images/lumerico284/post/6e73cf2f-43c9-46dd-9ef1-97b3e4bf06d9/image.png" width="60%">
</p>

### í•˜ì´í¼íŒŒë¼ë¯¸í„°

CelebA í•™ìŠµì— ì‚¬ìš©í•œ í•µì‹¬ ì„¤ì •ê°’ì€ `src/configs/celeba.yml` ê¸°ì¤€ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ë‹¤.

#### ê³µí†µ

| í•­ëª© | í‚¤ | ê°’ |
|---|---|---|
| ì‹¤í–‰ ë””ë°”ì´ìŠ¤ | `device` | `mps` |
| ì‹œë“œ | `seed` | `42` |

#### ë°ì´í„°

| í•­ëª© | í‚¤ | ê°’ |
|---|---|---|
| ë°ì´í„° ê²½ë¡œ | `data.root` | `data/celeba_32x32` |
| ì…ë ¥ í•´ìƒë„ | `data.image_size` | `32` |
| ì¢Œìš° ë°˜ì „ | `data.random_horizontal_flip` | `true` |
| ë¡œë” ì›Œì»¤ ìˆ˜ | `data.num_workers` | `4` |
| ë°ì´í„°ì…‹ í´ë˜ìŠ¤ | `data.dataset_cls` | `datasets.CelebAImageFolder` |

#### ëª¨ë¸

| í•­ëª© | í‚¤ | ê°’ |
|---|---|---|
| ì…ë ¥ ì±„ë„ | `model.in_channels` | `3` |
| ì±„ë„ í­(nf) | `model.nf` | `64` |
| ë…¸ì´ì¦ˆ ë ˆë²¨ ìˆ˜(K) | `model.num_scales` | `10` |
| ìµœëŒ€ ë…¸ì´ì¦ˆ | `model.sigma_begin` | `1.0` |
| ìµœì†Œ ë…¸ì´ì¦ˆ | `model.sigma_end` | `0.01` |
| dilation ì„¤ì • | `model.dilations` | `[1, 2, 4, 8]` |
| $\sigma$ ìŠ¤ì¼€ì¼ë§ | `model.scale_by_sigma` | `true` |

#### í•™ìŠµ

| í•­ëª© | í‚¤ | ê°’ |
|---|---|---|
| ë°°ì¹˜ í¬ê¸° | `training.batch_size` | `64` |
| ì—í­ ìˆ˜ | `training.epochs` | `100` |
| í•™ìŠµë¥  | `training.lr` | `0.0002` |
| Adam betas | `training.betas` | `[0.9, 0.999]` |
| ê°€ì¤‘ì¹˜ ê°ì‡  | `training.weight_decay` | `0.0` |
| ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦½ | `training.grad_clip` | `1.0` |
| ìë™ ì¬ê°œ | `training.auto_resume` | `true` |
| ì €ì¥ ì£¼ê¸° | `training.save_every` | `1` |
| ìƒ˜í”Œë§ ì£¼ê¸° | `training.sample_every` | `1` |

#### ìƒ˜í”Œë§(ALD)

| í•­ëª© | í‚¤ | ê°’ |
|---|---|---|
| ìƒ˜í”Œ ìˆ˜ | `sampling.n_samples` | `64` |
| ë ˆë²¨ë‹¹ ìŠ¤í… ìˆ˜ | `sampling.n_steps_each` | `100` |
| step lr | `sampling.step_lr` | `2.0e-05` |
| ê°’ í´ë¨í”„ | `sampling.clamp` | `true` |
| ë””ë…¸ì´ì¦ˆ ë‹¨ê³„ | `sampling.denoise` | `false` |
| ì´ˆê¸° ë¶„í¬ | `sampling.init_distribution` | `normal` |

### ì†ì‹¤ ê³¡ì„ 

<p align="center">
  <img src="https://velog.velcdn.com/images/lumerico284/post/5d5dee5f-a528-4c51-ae41-ddf36486ed78/image.png" width="80%">
</p>

Running loss ê³¡ì„ ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì‚´í´ë³´ë©´ ì „ì²´ì ìœ¼ë¡œ í•™ìŠµì´ ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆë‹¤ëŠ” ëŠë‚Œì„ ë°›ì•˜ì§€ë§Œ, ìµœì¢… ìƒ˜í”Œë§ ê²°ê³¼ë¥¼ í•¨ê»˜ ë†“ê³  ë³´ë©´ ì†ì‹¤ì´ plateauì— ë„ë‹¬í–ˆë‹¤ê³  í•´ì„œ ëª¨ë¸ì´ ì™„ì „íˆ ìˆ˜ë ´í–ˆë‹¤ê³  ë³´ê¸°ëŠ” ì–´ë µë‹¤ëŠ” ì ì´ ëª…í™•í•´ì¡Œë‹¤. ì´ˆë°˜ì—ëŠ” ì†ì‹¤ì´ ì•½ $7000$ì—ì„œ ë¹ ë¥´ê²Œ $1000$ ê·¼ì²˜ê¹Œì§€ ë–¨ì–´ì§€ë©´ì„œ ëª¨ë¸ì´ í° $\sigma$ êµ¬ê°„ì˜ ê±°ì¹œ êµ¬ì¡°ë¥¼ ë¹ ë¥´ê²Œ í•™ìŠµí•˜ëŠ” ì „í˜•ì ì¸ íŒ¨í„´ì„ ë³´ì˜€ê³ , ì´í›„ $600$ì—ì„œ $1000$ ì‚¬ì´ì˜ ì¢ì€ ë²”ìœ„ì—ì„œ ê¸´ êµ¬ê°„ ë™ì•ˆ ì§„ë™í•˜ë©° ì•ˆì •ì ì¸ plateauë¥¼ í˜•ì„±í–ˆë‹¤. ê²‰ë³´ê¸°ì—ëŠ” í•™ìŠµì´ ì˜ ì§„í–‰ë˜ê³  ìµœì í™”ê°€ ì•ˆì •ëœ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ì´ plateauì—ì„œì˜ ì•ˆì •í™”ê°€ ê³§ë°”ë¡œ ë°ì´í„° ë¶„í¬ì˜ ì¶©ë¶„í•œ ì»¤ë²„ë¦¬ì§€ë¥¼ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë¼ëŠ” ì ì„ ì´ë²ˆ ì‹¤í—˜ì—ì„œ ì²´ê°í–ˆë‹¤.

ì‹¤ì œë¡œ ìµœì¢… ìƒ˜í”Œë§ì„ ë³´ë©´ ì–¼êµ´ì˜ ì „ì²´ì ì¸ í˜•íƒœë‚˜ ì§ˆê°ì€ ì¡í˜€ ìˆì§€ë§Œ, íŠ¹ì • ìƒ‰ê°ì´ë‚˜ í—¤ì–´ í…ìŠ¤ì²˜ íŒ¨í„´ì´ ê³¼í•˜ê²Œ ë°˜ë³µë˜ê³ , í‘œì •ì´ë‚˜ ì–¼êµ´ êµ¬ì¡°ì˜ ë‹¤ì–‘ì„±ë„ ì œí•œì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ ì™„ì „í•œ ìˆ˜ë ´ê³¼ëŠ” ê±°ë¦¬ê°€ ìˆì—ˆë‹¤. ì¦‰, running lossëŠ” ì¼ì°ì´ í‰í‰í•´ì¡Œì§€ë§Œ, ê·¸ ê°’ ìì²´ê°€ ì•„ì§ ëª¨ë¸ì´ CelebAì˜ manifoldë¥¼ ì¶©ë¶„íˆ í•™ìŠµí–ˆë‹¤ê³  íŒë‹¨í•  ê¸°ì¤€ì´ ë˜ì§€ëŠ” ëª»í•œë‹¤ëŠ” ê²ƒì„ í™•ì¸í•œ ì…ˆì´ë‹¤. ì´ëŠ” DSM ì†ì‹¤ì´ ë³¸ì§ˆì ìœ¼ë¡œ $\mathbf{x} + \sigma \mathbf{z}$ í˜•íƒœì˜ ë…¸ì´ì¦ˆ ë³µì› ë¬¸ì œì— ì´ˆì ì„ ë‘ê¸° ë•Œë¬¸ì—, ì‹œê°ì  ë‹¤ì–‘ì„±ì´ë‚˜ ë¶„í¬ ì»¤ë²„ë¦¬ì§€ ê°™ì€ ìƒì„± í’ˆì§ˆê³¼ ì§ì ‘ì ìœ¼ë¡œ ì¼ëŒ€ì¼ ëŒ€ì‘í•˜ì§€ ì•ŠëŠ” íŠ¹ì„±ì´ ë°˜ì˜ëœ ê²°ê³¼ë¼ê³  ìƒê°í•œë‹¤.

ê²°êµ­ ì´ë²ˆ í•™ìŠµ ê³¡ì„ ì€ ìµœì í™”ê°€ ì•ˆì •ì ì´ê³  ìˆ˜ì¹˜ì ìœ¼ë¡œ í­ì£¼í•˜ì§€ëŠ” ì•Šì•˜ì§€ë§Œ, ì—¬ì „íˆ ë” ë§ì€ iterationì´ë‚˜ ë” ì„¸ë°€í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ í•„ìš”í•˜ë‹¤ëŠ” ì‹ í˜¸ë¡œ í•´ì„í•˜ëŠ” ê²ƒì´ ë§ëŠ” ê²ƒ ê°™ë‹¤. íŠ¹íˆ $\sigma$ ìŠ¤ì¼€ì¤„ì˜ ë²”ìœ„, `n_steps_each`, `step_lr`, ëª¨ë¸ ìš©ëŸ‰ ë“±ì„ ì¡°ì •í•´ì•¼ ë°ì´í„° ë¶„í¬ì˜ ë‹¤ì–‘í•œ ì˜ì—­ì„ ì œëŒ€ë¡œ ì¬í˜„í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ë‹¤ì‹œ ë§í•´, ì†ì‹¤ ê·¸ë˜í”„ë§Œ ë³´ë©´ í•™ìŠµì´ ì´ë¯¸ ì¶©ë¶„íˆ ëë‚œ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ì‹¤ì œ ìƒ˜í”Œì€ ê·¸ë ‡ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ì´ë²ˆ ì‹¤í—˜ì€ í•™ìŠµ ì•ˆì •ì„±ê³¼ ë°ì´í„° ìˆ˜ë ´ì´ ë°˜ë“œì‹œ ê°™ì€ íƒ€ì´ë°ì— ì˜¤ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì„ ë¶„ëª…í•˜ê²Œ ë³´ì—¬ì£¼ëŠ” ê³¼ì •ì´ì—ˆë‹¤.

### ê²°ê³¼ ìƒ˜í”Œë§

| ì´ˆê¸° ë¶„í¬ | `seed=42` | `seed=10` |
|------|------|------|
| $\mathcal{U}(-1,1)\in\mathbb{R}^{N\times H\times W}$ | <p align="center"><img src="https://velog.velcdn.com/images/lumerico284/post/c6a43ef5-3986-4483-acf5-59e54c13fdd2/image.gif" width="70%"/></p> | <p align="center"><img src="https://velog.velcdn.com/images/lumerico284/post/e8b3a26b-a1c1-47d9-bc2f-17ccf88c20cb/image.gif" width="70%"/></p> |
| $\mathcal{N}(\mathbf{0},\mathbf{I})\in\mathbb{R}^{N\times H\times W}$ | <p align="center"><img src="https://velog.velcdn.com/images/lumerico284/post/0d8a3a83-474c-49f1-b06f-31397425ab49/image.gif" width="70%"/></p> | <p align="center"><img src="https://velog.velcdn.com/images/lumerico284/post/e6c4c7ef-9545-4802-98cf-fbd955bb52b8/image.gif" width="70%"/></p> |

---

## âœ… ê²°ë¡ 

ë³¸ êµ¬í˜„ì€ NCSNì˜ í•µì‹¬ êµ¬ì¡°ë¥¼ ì´ë¡ ì  ê´€ì ì—ì„œ ì¶©ì‹¤íˆ ì¬í˜„í•˜ë©°, DSM í•™ìŠµë¶€í„° ALD ìƒ˜í”Œë§ì— ì´ë¥´ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ë…¼ë¬¸ ì‹ê³¼ ì½”ë“œ êµ¬í˜„ ê°„ì— ì¼ê´€ëœ ëŒ€ì‘ ê´€ê³„ë¥¼ ê°–ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤. Conditional Instance Normalizationì„ í†µí•œ ë…¸ì´ì¦ˆ ì¡°ê±´í™”, Ïƒ-ìŠ¤ì¼€ì¼ ë³´ì • ë°©ì‹, RefineNet ê¸°ë°˜ ë°±ë³¸ êµ¬ì¡° ë“± ì£¼ìš” êµ¬ì„± ìš”ì†Œê°€ ê° ë…¸ì´ì¦ˆ ë ˆë²¨ì—ì„œì˜ score ê·¼ì‚¬ë¥¼ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ë„ë¡ ì •êµí•˜ê²Œ êµ¬í˜„ë˜ì—ˆìœ¼ë©°, CelebA ì‹¤í—˜ì—ì„œë„ ë‹¤ì–‘í•œ ì´ˆê¸° ë¶„í¬ì—ì„œ ì¼ê´€ëœ ìƒ˜í”Œ íšŒë³µ ê³¼ì •ì„ ë³´ì—¬ score-based generative modelingì˜ ì‘ë™ ì›ë¦¬ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ë“œëŸ¬ë‚¬ë‹¤.

ë˜í•œ í•™ìŠµ ê³¡ì„ , ìƒ˜í”Œë§ ë™ì‘, í•˜ì´í¼íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¤„ì˜ íš¨ê³¼ ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ê´€ì°°í•¨ìœ¼ë¡œì¨ ë‹¤ì¤‘ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¼ì„ í™œìš©í•œ annealing ì ˆì°¨ê°€ ë°ì´í„° ë§¤ë‹ˆí´ë“œ ë³µì›ì— ì–´ë–»ê²Œ ê¸°ì—¬í•˜ëŠ”ì§€ë„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. ë¬´ì—‡ë³´ë‹¤ ì´ë²ˆ í”„ë¡œì íŠ¸ëŠ” â€œë™ì‘í•˜ëŠ” ì½”ë“œâ€ë¥¼ ë„˜ì–´, ì´ë¡ ì‹ â†’ êµ¬í˜„ â†’ ì‹¤í—˜ì  ë™ì‘ì˜ ì—°ê²°ì„ ëª…í™•íˆ ë“œëŸ¬ë‚´ëŠ” êµìœ¡ì Â·ì—°êµ¬ì  ê°€ì¹˜ê°€ ìˆëŠ” ì½”ë“œë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í–ˆë‹¤ëŠ” ì ì—ì„œ ì˜ë¯¸ê°€ í¬ë©°, ì´ëŠ” í–¥í›„ DDPM, NCSN++, Score-SDE ë“± í˜„ëŒ€ì  í™•ì‚°/ìŠ¤ì½”ì–´ ëª¨ë¸ ì—°êµ¬ë¡œ í™•ì¥í•  ìˆ˜ ìˆëŠ” ê²¬ê³ í•œ ê¸°ë°˜ì´ ë  ê²ƒì´ë‹¤.

---

#### ğŸ“„ ì¶œì²˜

Song, Yang, and Stefano Ermon. _"Generative Modeling by Estimating Gradients of the Data Distribution."_ Advances in Neural Information Processing Systems, vol. 32, 2019.